{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are three ways you can get and set Spark properties.\n",
    "\n",
    "<h3>1. In SPARK_HOME changing following files</h3>\n",
    "<ul>\n",
    "    <li>log4j.properties.template</li>\n",
    "    <li>spark-defaults.conf.template</li>\n",
    "    <li>spark-env.sh.template</li>\n",
    "    </ul>\n",
    "    \n",
    "If you rename files by deleting template and change values in it spark uses these values.\n",
    "\n",
    "<h3>2. During the spark-submit</h3>\n",
    "<p>You can change configuration by passing the new values with --conf flag</p>\n",
    "<code>spark-submit --conf spark.sql.shuffle.partitions=5 --conf\n",
    "\"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/mainscala-\n",
    "chapter7_2.12-1.0.jar</code>\n",
    "\n",
    "<h3>3. During the creation and after creation of SparkSession</h3>\n",
    "<p>Some propertes are not configurable after spark session get created. So you can just modify isModifiable properties after sparkSession.</p>\n",
    "\n",
    "<code>spark = SparkSession.builder\n",
    ".config(\"spark.sql.shuffle.partitions\", 5)\n",
    ".config(\"spark.driver.memory\", \"2g\")\n",
    ".master(\"local[*]\")\n",
    ".appName(\"SparkConfig\")\n",
    ".getOrCreate()</code>\n",
    "\n",
    "<h3>How to see all properties of Spark Application</h3>\n",
    "<ul>\n",
    "    <li>spark.conf.getAll()</li>\n",
    "    <li>Spark UI Environment Tab</li>\n",
    "    </ul>\n",
    "    \n",
    "<h3>Commandline has precedence over configuration files and code</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. log configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cp conf/log4j.properties.template conf/log4j.properties\n",
    "    Edit log4j.properties:\n",
    "\n",
    "    # Set everything to be logged to the console\n",
    "    log4j.rootCategory=INFO, console\n",
    "    log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
    "    log4j.appender.console.target=System.err\n",
    "    log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
    "    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
    "\n",
    "    # Settings to quiet third party logs that are too verbose\n",
    "    log4j.logger.org.eclipse.jetty=WARN\n",
    "    log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\n",
    "    log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\n",
    "    log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n",
    "    \n",
    "    \n",
    "    Replace at the first line:\n",
    "\n",
    "    log4j.rootCategory=INFO, console\n",
    "    \n",
    "    by:\n",
    "\n",
    "    log4j.rootCategory=WARN, console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scaling Spark for Large Workloads</h3>\n",
    "<p>To avoid job failures due to resource starvation\n",
    "or gradual performance degradation, there are a handful of Spark configurations that\n",
    "you can enable or alter. These configurations affect three Spark components: </p>\n",
    "<ul>\n",
    "    <li>Spark driver</li>\n",
    "    <li>Executor</li>\n",
    "    <li>Shuffle service running on the executor</li>\n",
    "    </ul>\n",
    "    \n",
    "    \n",
    "<h3>Dynamic Resource Allocation</h3>\n",
    "<code>spark.dynamicAllocation.enabled true\n",
    " spark.dynamicAllocation.minExecutors 2\n",
    " spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    " spark.dynamicAllocation.maxExecutors 20\n",
    " spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Spark executors’ memory and the shuffle service\n",
    "\n",
    "<p>Although we use dynamic allocation we have to define a single executor memory and cpu cores. <strong>Dynamic resource allocation just gets and leaves executors. We cannot change executors memory or cpu core</strong>, what changes is only number of executors.</p>\n",
    "\n",
    "<p>Configuring Spark executors’ memory and the shuffle service.</p>\n",
    "\n",
    "<img src=\"../images/spark_memory_management-spark_yarn_memory_eng.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Execution memory is used for Spark shuffles, joins, sorts, and aggregations. Since different\n",
    "queries may require different amounts of memory, the fraction <strong>(spark.mem\n",
    "ory.fraction is 0.6 by default)</strong> of the available memory to dedicate to this can be\n",
    "tricky to tune but it’s easy to adjust. By contrast, storage memory is primarily used for\n",
    "caching user data structures and partitions derived from DataFrames.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomended properties for shuffle ops based on experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_recomended_props_for_shuffle_ops.jpg\"/>\n",
    "\n",
    "<p>Source: Learning Spark, O'Reilly, 2020</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Default system properties included when running spark-submit.\n",
      "# spark.master                     spark://master:7077\n",
      "# spark.eventLog.enabled           true\n",
      "# spark.eventLog.dir               hdfs://namenode:8021/directory\n",
      "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
      "# spark.driver.memory              5g\n",
      "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\n",
      "spark.driver.memory                         512m\n",
      "spark.shuffle.file.buffer                   1m\n",
      "spark.file.transferTo                       false\n",
      "spark.shuffle.unsafe.file.output.buffer     1m\n",
      "spark.io.compression.lz4.blockSize          512k\n",
      "spark.shuffle.service.index.cache.size      200m\n",
      "spark.shuffle.registration.timeout          12000ms\n",
      "spark.shuffle.registration.maxAttempts      5\n",
      "spark.sql.warehouse.dir                     /user/hive/warehouse\n"
     ]
    }
   ],
   "source": [
    "! cat /opt/manual/spark/conf/spark-defaults.conf | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_tasks_core_partitions.png\"/>\n",
    "\n",
    "Source: Learning Spark, O'Reilly, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How partitions are created?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>A contiguous collection of these blocks constitutes a partition. For example HDSF block size: 128 MB</li>\n",
    "    <li>Size of partitions: <code>spark.sql.files.maxPartitionBytes</code></li>\n",
    "    <li>Only one partition is processed by one executor at a time. So, initally spark splits data into partitions as the same number of total executor cores.</li>\n",
    "    <li>You can change partition number with repartition(n) during the read or after <code>df.repartition(20)</code></li>\n",
    "    <li>Finally, shuffle partitions are created during the shuffle stage. By default, the number\n",
    "of shuffle partitions is set to <code>200</code> in spark.sql.shuffle.partitions. You can <code>adjust this number depending on the size of the data set</code> you have, to reduce the amount of\n",
    "small partitions being sent across the network to executors’ tasks. <code>lower value such as the number of cores on the executors or less</code></li>  \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Partition is the main unit of parallelism in Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under repartitioning meant the operation to reduce or increase the number of partitions in which the data in the cluster will be split. This process involves a full shuffle. Consequently, it is clear that <strong>repartitioning is an expensive process</strong>. In a typical scenario, most of the data should be serialized, moved, and deserialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repart_df = df.repartition(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to specifying the number of partitions directly, you can pass in the name of the column by which you want to partition the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartitioned = transactions.repartition('country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalesce\n",
    "The second way to manage partitions is coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation reduces the number of partitions and avoids a full shuffle. The executor can safely leave data on a minimum number of partitions, moving data only from redundant nodes. Therefore, it is better to use coalesce than repartition if you need to reduce the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_two_part = df.coalesce(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://luminousmen.com/post/spark-partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
