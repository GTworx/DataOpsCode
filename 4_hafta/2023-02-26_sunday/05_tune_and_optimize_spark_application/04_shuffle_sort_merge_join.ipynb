{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Similar to relational databases, the Spark DataFrame and Dataset APIs\n",
    "and Spark SQL offer a series of join transformations: inner joins, outer joins, left\n",
    "joins, right joins, etc. All of these operations trigger a large amount of data movement\n",
    "across Spark executors.</p>\n",
    "\n",
    "<h3>Five join strategiy</h3>\n",
    "<ol>\n",
    "    <li>The broadcast hash join (BHJ)</li>\n",
    "    <li>Shuffle hahs join (SHJ)</li>\n",
    "    <li>Shuffle sort merge join (SMJ)</li>\n",
    "    <li>Broadcast nested loop join (BNLJ)</li>\n",
    "    <li>Shuffle and replicated nested loop join (Castesian product)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Shuffle Hash Join (SHJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle Hash Join is divided into two steps:\n",
    "\n",
    "1.      On the two tables were in accordance with the join keys re-zoning, that shuffle, the purpose is to have the same join keys value of the record assigned to the corresponding partition\n",
    "\n",
    "2.       The corresponding partition in the data for the join, here first small table partition is constructed as a hash table, and then according to the large table recorded in the join keys value out to match.\n",
    "3.       If you can't use broadcast join and your tables fairly big enough you use SHJ.\n",
    "4.       One of the two tables are expected to be smaller than the other.\n",
    "5.       Join keys might not be sortable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Shuffle sort merge join (SMJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> By default Spark will use a <strong>broadcast join</strong> if the smaller data set is less than 10 MB. <br>\n",
    "This configuration is set in spark.sql.autoBroadcastJoinThreshold; <br>\n",
    "# By setting this value to -1 broadcasting can be disabled. <br>\n",
    "    We use <strong>Sort Merge Joins if we are joining two big tables.</strong> This join scheme has two phases: a sort phase followed by a merge phase. If you use buckets on join columns performance will increase due to preordering the columns.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 139844\n",
      "-rw-rw-r--. 1 train train     4556 Jul 21  2020 Advertising.csv\n",
      "-rw-rw-r--. 1 train train   674856 Sep 24 19:59 Churn_Modelling.csv\n",
      "drwxr-xr-x. 3 train train       96 Nov 19  2020 churn-telecom\n",
      "-rw-rw-r--. 1 train train 41002480 Aug 27 19:45 Fire_Incidents.csv.gz\n",
      "-rw-rw-r--. 1 train train 46401315 Aug 27 11:25 Hotel_Reviews.csv.gz\n",
      "-rw-rw-r--. 1 train train     4611 Nov 20  2020 iris.csv\n",
      "drwxr-xr-x. 2 train train      198 Jul 19 12:15 iris_parquet\n",
      "-rw-rw-r--. 1 train train     4365 Aug 27 12:29 Mall_customers\n",
      "-rw-rw-r--. 1 train train     4365 Aug 27 11:20 Mall_Customers.csv\n",
      "drwxrwxr-x. 2 train train      133 Jul 23  2020 retail_db\n",
      "drwxr-xr-x. 2 train train     4096 Jul 20 12:27 salary_avro\n",
      "drwxr-xr-x. 2 train train     4096 Jul 20 12:29 salary_json\n",
      "drwxr-xr-x. 2 train train     4096 Jul 20 12:28 salary_orc\n",
      "-rw-rw-r--. 1 train train      592 Jul 22 10:04 simple_data.csv\n",
      "-rw-rw-r--. 1 train train 40044293 Sep 19  2019 tmdb_5000_credits.csv\n",
      "-rw-rw-r--. 1 train train  9317430 Sep 14 23:28 tmdb_5000_movies._and_credits.zip\n",
      "-rw-rw-r--. 1 train train  5698602 Sep 19  2019 tmdb_5000_movies.csv\n"
     ]
    }
   ],
   "source": [
    "! ls -l ~/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/manual/spark\")\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/manual/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2022-09-24 21:55:19,251 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "SparkSession.builder\n",
    "    .appName(\"Joins\")\n",
    "    .master(\"local[2]\")\n",
    "    .config(\"spark.driver.memory\",\"3g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable broadcast hash join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "order_items = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".load(\"file:///home/train/datasets/retail_db/order_items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderItemName</th>\n",
       "      <th>orderItemOrderId</th>\n",
       "      <th>orderItemProductId</th>\n",
       "      <th>orderItemQuantity</th>\n",
       "      <th>orderItemSubTotal</th>\n",
       "      <th>orderItemProductPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>957</td>\n",
       "      <td>1</td>\n",
       "      <td>299.98</td>\n",
       "      <td>299.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1073</td>\n",
       "      <td>1</td>\n",
       "      <td>199.99</td>\n",
       "      <td>199.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>502</td>\n",
       "      <td>5</td>\n",
       "      <td>250.00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orderItemName  orderItemOrderId  orderItemProductId  orderItemQuantity  \\\n",
       "0              1                 1                 957                  1   \n",
       "1              2                 2                1073                  1   \n",
       "2              3                 2                 502                  5   \n",
       "\n",
       "   orderItemSubTotal  orderItemProductPrice  \n",
       "0             299.98                 299.98  \n",
       "1             199.99                 199.99  \n",
       "2             250.00                  50.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_items.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".load(\"file:///home/train/datasets/retail_db/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderId</th>\n",
       "      <th>orderDate</th>\n",
       "      <th>orderCustomerId</th>\n",
       "      <th>orderStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11599</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>256</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>12111</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>8827</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11318</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   orderId              orderDate  orderCustomerId      orderStatus\n",
       "0        1  2013-07-25 00:00:00.0            11599           CLOSED\n",
       "1        2  2013-07-25 00:00:00.0              256  PENDING_PAYMENT\n",
       "2        3  2013-07-25 00:00:00.0            12111         COMPLETE\n",
       "3        4  2013-07-25 00:00:00.0             8827           CLOSED\n",
       "4        5  2013-07-25 00:00:00.0            11318         COMPLETE"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssmerge_join_df = orders.join(order_items, \n",
    "                                orders.orderId == order_items.orderItemOrderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [orderId#50], [orderItemOrderId#17], Inner\n",
      ":- *(2) Sort [orderId#50 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(orderId#50, 200), ENSURE_REQUIREMENTS, [id=#79]\n",
      ":     +- *(1) Filter isnotnull(orderId#50)\n",
      ":        +- FileScan csv [orderId#50,orderDate#51,orderCustomerId#52,orderStatus#53] Batched: false, DataFilters: [isnotnull(orderId#50)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/datasets/retail_db/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(orderId)], ReadSchema: struct<orderId:int,orderDate:string,orderCustomerId:int,orderStatus:string>\n",
      "+- *(4) Sort [orderItemOrderId#17 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(orderItemOrderId#17, 200), ENSURE_REQUIREMENTS, [id=#87]\n",
      "      +- *(3) Filter isnotnull(orderItemOrderId#17)\n",
      "         +- FileScan csv [orderItemName#16,orderItemOrderId#17,orderItemProductId#18,orderItemQuantity#19,orderItemSubTotal#20,orderItemProductPrice#21] Batched: false, DataFilters: [isnotnull(orderItemOrderId#17)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/datasets/retail_db/order_items.csv], PartitionFilters: [], PushedFilters: [IsNotNull(orderItemOrderId)], ReadSchema: struct<orderItemName:int,orderItemOrderId:int,orderItemProductId:int,orderItemQuantity:int,orderI...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssmerge_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you from the plan spark sorts the join keys then executes join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Broadcast nested loop join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Broadcast nested loop join - In nested join for each row of first data set is iterate over every row of other dataset which may degrade performance in join operation.But in certain situation like join keys are not fixed as well as the query is qualified as broadcastable or not, according to the data statistics (size or broadcast hint). If neither of them is evaluated as true and the join type is inner, the query is executed with CartesianProductExec. In this cases BroadcastNestedLoopJoinExec is taken. One of the places where nested loop join is used independently on the dataset size is cross join resulting on cartesian product. In this situation each row from the left table is returned together with every row from the right table, if there is no predicate defined. Apache Spark provides a support for such type of queries with org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec physical operator. It's used when neither broadcast hash join nor shuffled hash join nor sort merge join can be used to execute the join statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
