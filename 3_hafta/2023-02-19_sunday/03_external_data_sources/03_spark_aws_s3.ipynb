{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"/opt/manual/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downlad Spark Hadoop AWS connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download following jars to /opt/manual/spark/jars\n",
    "\n",
    "# wget -P /opt/manual/spark/jars/ \\\n",
    "# https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar\n",
    "\n",
    "# wget -P /opt/manual/spark/jars/ \\\n",
    "# https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".master(\"local[2]\") \\\n",
    ".appName(\"Spark AWS S3\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare AWS S3 Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    cd ~\n",
    "    sudo yum -y install unzip\n",
    "    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "    unzip awscliv2.zip\n",
    "    sudo ./aws/install\n",
    "\n",
    "    aws configure\n",
    "    AWS Access Key ID [None]: <your key here>\n",
    "    AWS Secret Access Key [None]: <your secret key here>\n",
    "    Default region name [None]: eu-central-1\n",
    "    Default output format [None]: json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read AWS S3 Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "accessKeyId = ''\n",
    "secretAccessKey = ''\n",
    "\n",
    "config = configparser.RawConfigParser()\n",
    "\n",
    "config.read('/home/train/.aws/credentials')\n",
    "config.sections()\n",
    "accessKeyId = config.get('default', 'aws_access_key_id') ##configparser.NoSectionError: No section: 'default'\n",
    "secretAccessKey = config.get('default', 'aws_secret_access_key') ##configparser.NoSectionError: No section: 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 related Spark configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.access.key', accessKeyId)\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.secret.key', secretAccessKey)\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.path.style.access', 'true')\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "    spark_context._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.amazonaws.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -P ~/datasets https://raw.githubusercontent.com/erkansirin78/datasets/master/simple_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload it to s3 bucket from web console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".option(\"header\", True) \\\n",
    ".csv('s3a://vbo-de-input/simple_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+--------+--------+-----------+\n",
      "|sirano| isim|yas|  meslek|   sehir|aylik_gelir|\n",
      "+------+-----+---+--------+--------+-----------+\n",
      "|     1|Cemal| 35|    Isci|  Ankara|       3500|\n",
      "|     2|Ceyda| 42|   Memur| Kayseri|       4200|\n",
      "|     3|Timur| 30|Müzisyen|Istanbul|       9000|\n",
      "+------+-----+---+--------+--------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---+-----------+--------+-----------+------------------+\n",
      "|sirano|    isim|yas|     meslek|   sehir|aylik_gelir|        salary_dol|\n",
      "+------+--------+---+-----------+--------+-----------+------------------+\n",
      "|     1|   Cemal| 35|       Isci|  Ankara|       3500|259.25925925925924|\n",
      "|     2|   Ceyda| 42|      Memur| Kayseri|       4200| 311.1111111111111|\n",
      "|     3|   Timur| 30|   Müzisyen|Istanbul|       9000| 666.6666666666666|\n",
      "|     4|   Burcu| 29|Pazarlamaci|  Ankara|       4200| 311.1111111111111|\n",
      "|     5| Yasemin| 23|       null|   Bursa|       4800|355.55555555555554|\n",
      "|     6|     Ali| 33|      Memur|  Ankara|       4250|314.81481481481484|\n",
      "|     7|   Dilek| 29|Pazarlamaci|Istanbul|       7300| 540.7407407407408|\n",
      "|     8|   Murat| 31|   Müzisyen|Istanbul|      12000| 888.8888888888889|\n",
      "|     9|   Ahmet| 33|     Doktor|  Ankara|      18000|1333.3333333333333|\n",
      "|    10|Muhittin| 46|     Berber|Istanbul|      12000| 888.8888888888889|\n",
      "|    11|Hicaziye| 47| Tuhafiyeci|  Ankara|       4800|355.55555555555554|\n",
      "|    12|   Harun| 43|    Tornacı|  Ankara|       4200| 311.1111111111111|\n",
      "|    13|   Hakkı| 33|      Memur|   Çorum|       3750|277.77777777777777|\n",
      "|    14| Gülizar| 37|     Doktor|   İzmir|      14250|1055.5555555555557|\n",
      "|    15|  Şehmuz| 41|       null|  Ankara|       8700| 644.4444444444445|\n",
      "|    16|  Gençay| 46|     Berber|  Ankara|       8800| 651.8518518518518|\n",
      "|    16|  Gençay| 46|     Berber|  Ankara|       8800| 651.8518518518518|\n",
      "+------+--------+---+-----------+--------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"salary_dol\", F.col(\"aylik_gelir\") / 13.5)\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 22:26:33,799 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "2022-10-24 22:26:35,136 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.write.format('csv').option('header','true') \\\n",
    ".save('s3a://vbo-de-output/simple_data', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read from AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = spark.read.option('header','true') \\\n",
    ".csv('s3a://vbo-de-output/simple_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+--------+--------+-----------+------------------+\n",
      "|sirano| isim|yas|  meslek|   sehir|aylik_gelir|        salary_dol|\n",
      "+------+-----+---+--------+--------+-----------+------------------+\n",
      "|     1|Cemal| 35|    Isci|  Ankara|       3500|259.25925925925924|\n",
      "|     2|Ceyda| 42|   Memur| Kayseri|       4200| 311.1111111111111|\n",
      "|     3|Timur| 30|Müzisyen|Istanbul|       9000| 666.6666666666666|\n",
      "+------+-----+---+--------+--------+-----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to S3 web ui and check data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
