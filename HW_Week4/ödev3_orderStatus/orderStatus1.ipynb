{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Read the input file\n",
    "df = spark.read.csv(\"path/to/input/file.csv\", header=True)\n",
    "\n",
    "# Convert the date and time columns to timestamps\n",
    "df = df.withColumn(\"STATUS_DATETIME\", to_timestamp(concat_ws(\" \", df.STATUS_DATE, df.STATUS_TIME), \"yyyyMMdd HHmmss\"))\n",
    "\n",
    "# Create a window partitioned by ORDER_ID and sorted by STATUS_DATETIME\n",
    "window_spec = Window.partitionBy(\"ORDER_ID\").orderBy(\"STATUS_DATETIME\")\n",
    "\n",
    "# Create a column for the previous status\n",
    "df = df.withColumn(\"PREV_STATUS\", lag(\"STATUS\", 1).over(window_spec))\n",
    "\n",
    "# Create columns for the start date and end date\n",
    "df = df.withColumn(\"START_DATE\", when((df.STATUS == \"CREATED\") | (df.STATUS == \"POOL\"), df.STATUS_DATETIME))\n",
    "df = df.withColumn(\"END_DATE\", when((df.STATUS == \"COMPLETED\") | (df.STATUS == \"CANCELLED\"), df.STATUS_DATETIME))\n",
    "\n",
    "# Create a column for the current status\n",
    "df = df.withColumn(\"CURRENT_STATUS\", when(df.END_DATE.isNotNull(), df.STATUS).otherwise(df.PREV_STATUS))\n",
    "\n",
    "# Create a column for the duration\n",
    "df = df.withColumn(\"DURATION\", when(df.END_DATE.isNotNull(), unix_timestamp(df.END_DATE) - unix_timestamp(df.START_DATE)).otherwise(0))\n",
    "\n",
    "# Select the desired columns and write the output to a file\n",
    "output_df = df.select(\"ORDER_ID\", \"CURRENT_STATUS\", \"START_DATE\", \"END_DATE\", \"DURATION\")\n",
    "output_df.write.csv(\"path/to/output/file.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Read the input file\n",
    "df = spark.read.csv(\"path/to/input/file.csv\", header=True)\n",
    "\n",
    "# Convert the date and time columns to timestamps\n",
    "df = df.withColumn(\"STATUS_DATETIME\", to_timestamp(concat_ws(\" \", df.STATUS_DATE, df.STATUS_TIME), \"yyyyMMdd HHmmss\"))\n",
    "\n",
    "# Create a window partitioned by ORDER_ID and sorted by STATUS_DATETIME\n",
    "window_spec = Window.partitionBy(\"ORDER_ID\").orderBy(\"STATUS_DATETIME\")\n",
    "\n",
    "# Create a column for the previous status\n",
    "df = df.withColumn(\"PREV_STATUS\", lag(\"STATUS\", 1).over(window_spec))\n",
    "\n",
    "# Create columns for the start date and end date\n",
    "df = df.withColumn(\"START_DATE\", when((df.STATUS == \"CREATED\") | (df.STATUS == \"POOL\"), df.STATUS_DATETIME))\n",
    "df = df.withColumn(\"END_DATE\", when((df.STATUS == \"COMPLETED\") | (df.STATUS == \"CANCELLED\"), df.STATUS_DATETIME))\n",
    "\n",
    "# Create a column for the current status\n",
    "df = df.withColumn(\"CURRENT_STATUS\", when(df.END_DATE.isNotNull(), df.STATUS).otherwise(df.PREV_STATUS))\n",
    "\n",
    "# Create a column for the duration\n",
    "df = df.withColumn(\"DURATION\", when(df.END_DATE.isNotNull(), unix_timestamp(df.END_DATE) - unix_timestamp(df.START_DATE)).otherwise(0))\n",
    "\n",
    "# Select the desired columns and write the output to a file\n",
    "output_df = df.select(\"ORDER_ID\", \"CURRENT_STATUS\", \"START_DATE\", \"END_DATE\", \"DURATION\")\n",
    "output_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"path/to/output/file.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, min, max, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"OrderStatus\").getOrCreate()\n",
    "\n",
    "# Load the dataset into a Spark dataframe\n",
    "df = spark.read.csv(\"https://raw.githubusercontent.com/GTworx/datasets/main/orderStatusData.csv\", header=True)\n",
    "\n",
    "# Convert the time columns to timestamps\n",
    "df = df.withColumn(\"EVENT_TIMESTAMP\", df[\"EVENT_TIMESTAMP\"].cast(\"timestamp\"))\n",
    "df = df.withColumn(\"EVENT_TIME\", df[\"EVENT_TIME\"].cast(\"timestamp\"))\n",
    "\n",
    "# Pivot the dataframe to get the start and end times for each status\n",
    "pivot = df.groupBy(\"ORDER_ID\", \"SUBSCRIBER_ID\").pivot(\"STATUS\").agg(min(\"EVENT_TIMESTAMP\"), max(\"EVENT_TIME\"))\n",
    "\n",
    "# Calculate the start and end times based on the CREATED and POOL statuses\n",
    "start_date = when(col(\"CREATED\") < col(\"POOL\"), col(\"CREATED\")).otherwise(col(\"POOL\"))\n",
    "end_date = when(col(\"COMPLETED\").isNotNull(), col(\"COMPLETED\")).otherwise(col(\"CANCELLED\"))\n",
    "\n",
    "# Calculate the duration in seconds\n",
    "duration = (end_date.cast(\"long\") - start_date.cast(\"long\")).cast(DoubleType())/60\n",
    "\n",
    "# Create the result dataframe\n",
    "result = pivot.select(\n",
    "    col(\"ORDER_ID\"),\n",
    "    col(\"SUBSCRIBER_ID\"),\n",
    "    col(\"CREATED\").alias(\"START_DATE\"),\n",
    "    end_date.alias(\"END_DATE\"),\n",
    "    when(duration > 0, duration).otherwise(0).alias(\"DURATION\"),\n",
    "    when(end_date.isNotNull(), \"COMPLETED\").otherwise(col(\"POOL\")).alias(\"STATUS\")\n",
    ").orderBy(\"ORDER_ID\")\n",
    "\n",
    "# Show the result dataframe\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, min, max, to_timestamp, col\n",
    "\n",
    "# Assuming the input DataFrame is called \"orders_df\" and has the columns:\n",
    "# ORDER_ID, SUBSCRIBER_ID, STATUS, START_DATE, END_DATE, DURATION\n",
    "\n",
    "# Convert the START_DATE and END_DATE columns to timestamps\n",
    "orders_df = orders_df.withColumn(\"START_DATE\", to_timestamp(col(\"START_DATE\")))\n",
    "orders_df = orders_df.withColumn(\"END_DATE\", to_timestamp(col(\"END_DATE\")))\n",
    "\n",
    "# Create a new column for the minimum start date of CREATED and POOL statuses\n",
    "min_start_date = when(col(\"STATUS\").isin([\"CREATED\", \"POOL\"]), col(\"START_DATE\")).otherwise(None)\n",
    "orders_df = orders_df.withColumn(\"MIN_START_DATE\", min_start_date.over(Window.partitionBy(\"ORDER_ID\")))\n",
    "\n",
    "# Create a new column for the maximum end date of COMPLETED and CANCELLED statuses\n",
    "max_end_date = when(col(\"STATUS\").isin([\"COMPLETED\", \"CANCELLED\"]), col(\"END_DATE\")).otherwise(None)\n",
    "orders_df = orders_df.withColumn(\"MAX_END_DATE\", max_end_date.over(Window.partitionBy(\"ORDER_ID\")))\n",
    "\n",
    "# Calculate the duration as the difference between the minimum start date and maximum end date\n",
    "orders_df = orders_df.withColumn(\"DURATION\", (col(\"MAX_END_DATE\").cast(\"long\") - col(\"MIN_START_DATE\").cast(\"long\")) / 3600.0)\n",
    "\n",
    "# Select only the necessary columns and drop duplicates\n",
    "result_df = orders_df.select(\"ORDER_ID\", \"SUBSCRIBER_ID\", \"STATUS\", \"MIN_START_DATE\", \"MAX_END_DATE\", \"DURATION\") \\\n",
    "                     .dropDuplicates([\"ORDER_ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Load sales orders data into a DataFrame\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_orders AS\n",
    "SELECT *\n",
    "FROM <sales_orders_table>\n",
    "\n",
    "-- Set start date and end date based on order status\n",
    "SELECT\n",
    "  ORDER_ID,\n",
    "  SUBSCRIBER_ID,\n",
    "  STATUS,\n",
    "  CASE\n",
    "    WHEN STATUS = 'CREATED' THEN LEAST(CREATED_DATE, POOL_DATE)\n",
    "    ELSE NULL\n",
    "  END AS START_DATE,\n",
    "  CASE\n",
    "    WHEN STATUS IN ('COMPLETED', 'CANCELLED') THEN GREATEST(CANCELLED_DATE, COMPLETED_DATE)\n",
    "    ELSE NULL\n",
    "  END AS END_DATE,\n",
    "  0 AS DURATION\n",
    "FROM sales_orders\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  ORDER_ID,\n",
    "  SUBSCRIBER_ID,\n",
    "  STATUS,\n",
    "  CASE\n",
    "    WHEN STATUS = 'CREATED' THEN LEAST(CREATED_DATE, POOL_DATE)\n",
    "    ELSE NULL\n",
    "  END AS START_DATE,\n",
    "  CASE\n",
    "    WHEN STATUS IN ('COMPLETED', 'CANCELLED') THEN GREATEST(CANCELLED_DATE, COMPLETED_DATE)\n",
    "    ELSE NULL\n",
    "  END AS END_DATE,\n",
    "  DATEDIFF(SECOND, LEAST(CREATED_DATE, POOL_DATE), GREATEST(CANCELLED_DATE, COMPLETED_DATE)) AS DURATION\n",
    "FROM sales_orders\n",
    "WHERE STATUS IN ('COMPLETED', 'CANCELLED')\n",
    "\n",
    "-- Group by ORDER_ID to get max duration and latest end date for each order\n",
    "GROUP BY ORDER_ID\n",
    "SELECT\n",
    "  ORDER_ID,\n",
    "  MAX(DURATION) AS DURATION,\n",
    "  MAX(END_DATE) AS END_DATE\n",
    "FROM (\n",
    "  -- Combine the two sets of data to get all orders, including those that are not completed or cancelled\n",
    "  SELECT * FROM <first_set_of_data>\n",
    "  UNION ALL\n",
    "  SELECT * FROM <second_set_of_data>\n",
    ")\n",
    "GROUP BY ORDER_ID\n",
    "\n",
    "-- Join with the original DataFrame to get the current status\n",
    "SELECT\n",
    "  sales_orders.ORDER_ID,\n",
    "  sales_orders.SUBSCRIBER_ID,\n",
    "  latest_status.STATUS,\n",
    "  latest_status.START_DATE,\n",
    "  latest_status.END_DATE,\n",
    "  latest_status.DURATION\n",
    "FROM sales_orders\n",
    "JOIN (\n",
    "  SELECT\n",
    "    ORDER_ID,\n",
    "    STATUS,\n",
    "    START_DATE,\n",
    "    END_DATE,\n",
    "    DURATION\n",
    "  FROM (\n",
    "    -- Rank the rows within each ORDER_ID by END_DATE in descending order\n",
    "    SELECT\n",
    "      *,\n",
    "      RANK() OVER (PARTITION BY ORDER_ID ORDER BY END_DATE DESC) AS row_rank\n",
    "    FROM <grouped_data>\n",
    "  ) ranked_data\n",
    "  WHERE row_rank = 1 -- Get the latest row for each ORDER_ID\n",
    ") latest_status\n",
    "ON sales_orders.ORDER_ID = latest_status.ORDER_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM <sales_orders_table>\n",
    "\n",
    "-- Set start date and end date based on order status\n",
    "SELECT\n",
    "  ORDER_ID,\n",
    "  SUBSCRIBER_ID,\n",
    "  STATUS,\n",
    "  CASE\n",
    "    WHEN STATUS = 'CREATED' THEN LEAST(CREATED_DATE, POOL_DATE)\n",
    "    ELSE NULL\n",
    "  END AS START_DATE,\n",
    "  CASE\n",
    "    WHEN STATUS IN ('COMPLETED', 'CANCELLED') THEN GREATEST(CANCELLED_DATE, COMPLETED_DATE)\n",
    "    ELSE NULL\n",
    "  END AS END_DATE,\n",
    "  0 AS DURATION\n",
    "FROM sales_orders\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  ORDER_ID,\n",
    "  SUBSCRIBER_ID,\n",
    "  STATUS,\n",
    "  CASE\n",
    "    WHEN STATUS = 'CREATED' THEN LEAST(CREATED_DATE, POOL_DATE)\n",
    "    ELSE NULL\n",
    "  END AS START_DATE,\n",
    "  CASE\n",
    "    WHEN STATUS IN ('COMPLETED', 'CANCELLED') THEN GREATEST(CANCELLED_DATE, COMPLETED_DATE)\n",
    "    ELSE NULL\n",
    "  END AS END_DATE,\n",
    "  DATEDIFF(SECOND, LEAST(CREATED_DATE, POOL_DATE), GREATEST(CANCELLED_DATE, COMPLETED_DATE)) AS DURATION\n",
    "FROM sales_orders\n",
    "WHERE STATUS IN ('COMPLETED', 'CANCELLED')\n",
    "\n",
    "-- Group by ORDER_ID to get max duration and latest end date for each order\n",
    "GROUP BY ORDER_ID\n",
    "SELECT\n",
    "  ORDER_ID,\n",
    "  MAX(DURATION) AS DURATION,\n",
    "  MAX(END_DATE) AS END_DATE\n",
    "FROM (\n",
    "  -- Combine the two sets of data to get all orders, including those that are not completed or cancelled\n",
    "  SELECT * FROM <first_set_of_data>\n",
    "  UNION ALL\n",
    "  SELECT * FROM <second_set_of_data>\n",
    ")\n",
    "GROUP BY ORDER_ID\n",
    "\n",
    "-- Join with the original DataFrame to get the current status\n",
    "SELECT\n",
    "  sales_orders.ORDER_ID,\n",
    "  sales_orders.SUBSCRIBER_ID,\n",
    "  latest_status.STATUS,\n",
    "  latest_status.START_DATE,\n",
    "  latest_status.END_DATE,\n",
    "  latest_status.DURATION\n",
    "FROM sales_orders\n",
    "JOIN (\n",
    "  SELECT\n",
    "    ORDER_ID,\n",
    "    STATUS,\n",
    "    START_DATE,\n",
    "    END_DATE,\n",
    "    DURATION\n",
    "  FROM (\n",
    "    -- Rank the rows within each ORDER_ID by END_DATE in descending order\n",
    "    SELECT\n",
    "      *,\n",
    "      RANK() OVER (PARTITION BY ORDER_ID ORDER BY END_DATE DESC) AS row_rank\n",
    "    FROM <grouped_data>\n",
    "  ) ranked_data\n",
    "  WHERE row_rank = 1 -- Get the latest row for each ORDER_ID\n",
    ") latest_status\n",
    "ON sales_orders.ORDER_ID = latest_status.ORDER_ID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a54084e6b208ee8d1ce3989ffc20924477a5f55f5a43e22e699a6741623861e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
